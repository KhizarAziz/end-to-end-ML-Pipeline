{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "kBCvM9LH_QQv",
        "iiWgEbwJ8pU4",
        "ZdkStWs5_JmT",
        "3kJFU4PIL6Zi",
        "GgD-cQg0sOEG",
        "HqOxco91j6VL"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**In this project, we will build a sentiment analysis using yelp and distillBERT**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "OZtx0eDC5kHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports and IDE Configs**"
      ],
      "metadata": {
        "id": "kBCvM9LH_QQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "_OQiXXUmRZcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "tVBE9gZZStxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments, DistilBertModel\n",
        "from datasets import Dataset #hugginface datasets"
      ],
      "metadata": {
        "id": "iQvr0QyF_Xc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Download dataset**"
      ],
      "metadata": {
        "id": "iiWgEbwJ8pU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/KhizarAziz/end-to-end-ML-Pipeline.git\n",
        "!mv end-to-end-ML-Pipeline/* ./\n",
        "!rm -rf end-to-end-ML-Pipeline"
      ],
      "metadata": {
        "id": "KfAZYHmq-qGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gg0ut1b9uEwt"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle/\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir datasets\n",
        "!python my_datasets/download_kaggle_dataset.py yelp-dataset/yelp-dataset --path my_datasets/"
      ],
      "metadata": {
        "id": "pdD8oUgU4Kja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf datasets/yelp-dataset.zip"
      ],
      "metadata": {
        "id": "ZUvSqzopD3lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load dataset and EDA**"
      ],
      "metadata": {
        "id": "ZdkStWs5_JmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size = 1000  # adjust based on available memory\n",
        "dataset_path = \"/content/my_datasets/yelp-dataset/yelp_academic_dataset_review.json\"\n",
        "json_reader = pd.read_json(dataset_path, lines=True, chunksize=chunk_size) # this get all the chunks of json file (as the file is too large, cannot load all at once)\n",
        "df = next(json_reader) # loading first chunk"
      ],
      "metadata": {
        "id": "GRr09Nfb_Ym_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('yelp_academic_dataset_review.csv', index=False)"
      ],
      "metadata": {
        "id": "7uP5Wqd5LkaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv('yelp_academic_dataset_review.csv')"
      ],
      "metadata": {
        "id": "vCRWPqJbKKLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "OKrmchxEVXXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "7KrfmiwzMWMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your DataFrame is named df and has a 'stars' column\n",
        "df['stars'].value_counts().sort_index().plot(kind='bar', title='Star Ratings Distribution')"
      ],
      "metadata": {
        "id": "XjDkiLzNKQQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Processing**"
      ],
      "metadata": {
        "id": "3kJFU4PIL6Zi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning\n"
      ],
      "metadata": {
        "id": "zE3KC3LoWywp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text cleaning function (customize as needed)\n",
        "def clean_text(text):\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www.\\S+', '[URL]', text)\n",
        "    # Remove Emails\n",
        "    text = re.sub(r'\\S+@\\S+', '[EMAIL]', text)\n",
        "    # Remove new line and line breaks\n",
        "    text = text.replace('\\n', ' ').replace('\\r', '').strip()\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Optional: Lowercase (Depends on whether your DistilBERT model is cased or not)\n",
        "    # text = text.lower()\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "kYpwRU8OPwMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filtering\n",
        "df.drop(columns=['review_id','user_id','business_id'],inplace=True)\n",
        "df.drop_duplicates(inplace=True)\n",
        "df.dropna(inplace=True)\n",
        "# Clean the 'text' column\n",
        "df['clean_text'] = df['text'].apply(clean_text)"
      ],
      "metadata": {
        "id": "ta-z-Y-bEs2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Feature Engineering"
      ],
      "metadata": {
        "id": "uYsXAY2-W32I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# our labels are number of stars which are 1-5 and model expects 0-N (0-4) --->>>> ordinal encoding. So...\n",
        "# kind of encoding (blkay actually encoding. just coincidentally numbers almost same hn)\n",
        "df['stars'] = df['stars'] - 1 # feature engineering"
      ],
      "metadata": {
        "id": "wYv4yytuW8IR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "N_XIRqtOXXgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# validation splitting\n",
        "train_df, val_df = train_test_split(df, test_size=0.2)"
      ],
      "metadata": {
        "id": "Xh-V8Ke8bUjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model needs int64 data and this stars originally were float.\n",
        "train_df['stars'] = train_df['stars'].astype(int)\n",
        "val_df['stars'] = val_df['stars'].astype(int)"
      ],
      "metadata": {
        "id": "sRj82OTdE39P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preparing huggingface datsets with expected column names (for the tokenizer and model)\n",
        "train_data = Dataset.from_pandas(train_df[['text', 'stars']].rename(columns={'text': 'text', 'stars': 'label'}))\n",
        "val_data = Dataset.from_pandas(val_df[['text', 'stars']].rename(columns={'text': 'text', 'stars': 'label'}))"
      ],
      "metadata": {
        "id": "BKCHfHGFa3N7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dump data into S3\n",
        "# Using Hugging Face's Datasets\n",
        "train_data.save_to_disk(\"./datasets/train_data/\")\n",
        "val_data.save_to_disk(\"./datasets/val_data/\")"
      ],
      "metadata": {
        "id": "-kuyt5_cPrSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Loading and Training**"
      ],
      "metadata": {
        "id": "GgD-cQg0sOEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True)"
      ],
      "metadata": {
        "id": "Yer8GnR4cnXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk\n",
        "train_data = load_from_disk(\"./datasets/train_data/\")\n",
        "val_data = load_from_disk(\"./datasets/val_data/\")"
      ],
      "metadata": {
        "id": "bm2D9Ge2fwIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize dataset\n",
        "train_data_tokenized = train_data.map(tokenize_function, batched=True)\n",
        "val_data_tokenized = val_data.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "u0oBz6nHbw4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hugging face's this model is in torch, so we need to convert the type to torch dataset.\n",
        "# Format for pytorch\n",
        "train_data_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "val_data_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
      ],
      "metadata": {
        "id": "z44sMH1ryE3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs'\n",
        ")\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(train_data_tokenized['label'].unique()))"
      ],
      "metadata": {
        "id": "id1zdvun4GEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer( #hugggingface trainer module takes care of parallel trainig.\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data_tokenized,\n",
        "    eval_dataset=val_data_tokenized\n",
        ")"
      ],
      "metadata": {
        "id": "rCoTDs6w9-J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels = len(train_data_tokenized['label'].unique())\n",
        "model_config_num_labels = model.config.num_labels\n",
        "\n",
        "print(f\"Dataset num_labels: {num_labels}\")\n",
        "print(f\"Model config num_labels: {model_config_num_labels}\")"
      ],
      "metadata": {
        "id": "LbG2DjWeK_1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "4SPlBERNVJOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save to s3\n",
        "model.save_pretrained(\"./models/\")"
      ],
      "metadata": {
        "id": "2QvebzpoD0ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script Testing"
      ],
      "metadata": {
        "id": "G6WE9O71mGmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/KhizarAziz/end-to-end-ML-Pipeline.git\n",
        "!mv end-to-end-ML-Pipeline/* ./"
      ],
      "metadata": {
        "id": "sF6WlYCVmJ_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing\n",
        "!python ./preprocessing/preprocess_yelp_review_dataset.py --dataset_path ./datasets/yelp-dataset/yelp_academic_dataset_review.json --chunk_size 1000"
      ],
      "metadata": {
        "id": "acjSgMHgpuhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "!python ./train.py --train_data_path ./datasets/yelp-dataset/train_set --val_data_path ./datasets/yelp-dataset/train_set"
      ],
      "metadata": {
        "id": "uXuFvc19xX_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2YWwa4_1Vxhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# p = \"/content/datasets/yelp-dataset/yelp_academic_dataset_review.json\"\n",
        "# '/'.join(p.split('/')[:-1])+ '/train_set/'\n",
        "# train_dumpyard_path = args.dataset_path.split('/')"
      ],
      "metadata": {
        "id": "YgTf574msccX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **inference**"
      ],
      "metadata": {
        "id": "HqOxco91j6VL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# text = \"Replace me by any text you'd like.\"\n",
        "# encoded_input = tokenizer(text, return_tensors='pt')\n",
        "# output = model(**encoded_input)"
      ],
      "metadata": {
        "id": "jb6JXCGD4NZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = [\"food is awesome!\",\"It was best\",\"I hate it\"]\n",
        "test_encodings = tokenizer(test_text, padding=True, truncation=True, return_tensors='pt')\n",
        "test_dataset = Dataset.from_dict(test_encodings)\n",
        "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "\n",
        "predictions = trainer.predict(test_dataset)\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "logits = torch.tensor(predictions.predictions)\n",
        "probabilities = F.softmax(logits, dim=0)\n",
        "print(probabilities)"
      ],
      "metadata": {
        "id": "n8cYYYg-P8mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tlt1S7tNgspM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "chs0-22Eh3ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions.predictions[0]),predictions.predictions[1]"
      ],
      "metadata": {
        "id": "wjvYgoAIg16O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.predictions[2]"
      ],
      "metadata": {
        "id": "8wALIt2AhaBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_label_idx = torch.argmax(predictions.predictions[0]).item()"
      ],
      "metadata": {
        "id": "UZTqJbZPgydW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pSgU_ZeSgzI4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}